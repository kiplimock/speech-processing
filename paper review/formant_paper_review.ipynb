{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b51693a5",
   "metadata": {},
   "source": [
    "### Visualizing The Audio File Spectrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b36bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.fft import rfft, rfftfreq\n",
    "import librosa\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "filepath = 'C:\\\\Users\\\\kipli\\\\Cedric\\\\speech_processing\\\\paper review\\\\SI681.WAV.wav'\n",
    "data, sr = librosa.load(filepath, sr=8000, res_type='fft')\n",
    "nfft = 256\n",
    "overlap = nfft/2\n",
    "frame_1 = data[0:256]\n",
    "\n",
    "formants = np.array([[250, 2250, 2890],[400, 1920, 2560],[550, 1770, 2490],[690, 1660, 2490],\n",
    "                     [710, 1100, 2540],[590, 880, 2540],[450, 1030, 2380],[310, 870, 2250]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c82a1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_fft(audio_data):\n",
    "    \n",
    "    y = rfft(audio_data)\n",
    "    x = rfftfreq(len(audio_data), 1/sr)\n",
    "    \n",
    "    plt.figure(figsize=(7,2))\n",
    "    plt.bar(x, np.abs(y)/np.max(np.abs(y)), 125)\n",
    "    plt.ylabel(\"\")\n",
    "    plt.xlabel(\"Frequency (Hz)\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f80469a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_fft(frame_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16beb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "IPython.display.Audio(data, rate=sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ca0d0d",
   "metadata": {},
   "source": [
    "### Neighbour Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88e321d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def neighbour_peak_search(frame):\n",
    "    # formants for each vowel\n",
    "    v_peaks = []\n",
    "    amplitudes = rfft(frame)\n",
    "    frequencies = rfftfreq(len(frame), 1/sr)\n",
    "    \n",
    "    # inside the formants array\n",
    "    for formant in formants:\n",
    "        # inside the vowel formants array peaks for each formant\n",
    "        fmt_freq = []\n",
    "        for freq in formant:\n",
    "            # minimum difference between the known formant frequencies and fft frequencies\n",
    "            base_peak_index = np.argmin([np.abs(freq - fft_freq) for fft_freq in frequencies])\n",
    "            modified_peak = np.max([amplitudes[base_peak_index - 4:base_peak_index + 4]])\n",
    "            modified_peak_index = np.where(amplitudes == modified_peak)[0][0]\n",
    "            fmt_freq.append(modified_peak_index)\n",
    "            \n",
    "        v_peaks.append(fmt_freq)\n",
    "    \n",
    "    return v_peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3cd3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbour_peak_search(frame_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ecfd0a",
   "metadata": {},
   "source": [
    "### Dividing the audio file into frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4a5828",
   "metadata": {},
   "outputs": [],
   "source": [
    "def frame_splits(data, framesize, overlap):\n",
    "    frames = []\n",
    "    for index in range(0, len(data)-framesize, int(overlap)):\n",
    "        frame = data[index:index + framesize]\n",
    "        frames.append(frame)\n",
    "    \n",
    "    return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5015ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# total number of frames in the audio file\n",
    "no_of_frames = len(frame_splits(data, nfft, overlap))\n",
    "print(f\"Number of frames: {no_of_frames}\")\n",
    "# number of samples per frame\n",
    "no_of_samples = len(frame_splits(data, nfft, overlap)[0])\n",
    "print(f\"Number of samples per frame: {no_of_samples}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca68b85",
   "metadata": {},
   "source": [
    "### Peak Neighbour Difference Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157eb7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def peak_neighbour_difference(audio_data):\n",
    "    \n",
    "    section_vowel_PND = [] # PND results for each spectral peak in the section\n",
    "    frames = frame_splits(audio_data, nfft, overlap) # outputs an array containing sections of the audio file\n",
    "    \n",
    "    for frame in frames:\n",
    "        section_fft_amplitudes = rfft(frame)\n",
    "        section_fft_frequencies = rfftfreq(len(frame))\n",
    "        vowel_peaks = neighbour_peak_search(frame) # peaks for the section/frame\n",
    "        \n",
    "        section_formant_PND = []\n",
    "        \n",
    "        for vowel_peak in vowel_peaks:\n",
    "            current = np.array([section_fft_amplitudes[index] for index in vowel_peak])\n",
    "            after = np.array([section_fft_amplitudes[index + 1] for index in vowel_peak])\n",
    "            before = np.array([section_fft_amplitudes[index - 1] for index in vowel_peak])\n",
    "            pnd_val = np.abs(current - after - before)\n",
    "            \n",
    "            section_formant_PND.append(list(pnd_val))\n",
    "        \n",
    "        section_vowel_PND.append(section_formant_PND)\n",
    "        \n",
    "    return section_vowel_PND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ad31b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pnd = peak_neighbour_difference(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc269e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pnd[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26452764",
   "metadata": {},
   "source": [
    "### Weighted PND\n",
    "Weights are applied to the calculated values of PND to boost the importance of some formants. For the paper reviewed, the weights used were:\n",
    "1. w<sub>1</sub> = 2.5 for the first formant\n",
    "2. w<sub>2</sub> = 1 for the second formant\n",
    "3. w<sub>3</sub> = 1 for the third formant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3eae1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vowels = ['vowel1', 'vowel2', 'vowel3', 'vowel4', 'vowel5', 'vowel6', 'vowel7', 'vowel8']\n",
    "def weighted_formant_pnd(audio_data):\n",
    "    weights = np.array([2.5, 1.0, 1.0])\n",
    "    pnd_values = peak_neighbour_difference(audio_data)\n",
    "    \n",
    "    weighted_pnd_values = []\n",
    "    for vowel_pnd in pnd_values:\n",
    "        weighted_formants = [sum(formant_signicance*weights) for formant_signicance in vowel_pnd]\n",
    "        pnd_vowel_pairs = list(zip(weighted_formants, vowels))\n",
    "        weighted_pnd_values.append(max(pnd_vowel_pairs))\n",
    "    \n",
    "    return weighted_pnd_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64300174",
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted = weighted_formant_pnd(data)\n",
    "weighted[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243b82b8",
   "metadata": {},
   "source": [
    "### Computing the Threshold Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bb1ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1.7\n",
    "beta = 7000\n",
    "\n",
    "def threshold():\n",
    "    summation = 0\n",
    "    for pair in weighted[:20]:\n",
    "        summation = summation + pair[0]\n",
    "        \n",
    "    threshold_value = summation/20 * alpha\n",
    "    return threshold_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737830f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "thres = threshold()\n",
    "thres"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f22fe3",
   "metadata": {},
   "source": [
    "### Speech Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9032788",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_speech():\n",
    "    pnd_vowel_pairs = weighted_formant_pnd(data)\n",
    "    \n",
    "    speech_data = []\n",
    "    for pair in pnd_vowel_pairs:\n",
    "        if pair[0] >= thres:\n",
    "            speech_data.append(pair)\n",
    "    \n",
    "    return speech_data\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a86493",
   "metadata": {},
   "outputs": [],
   "source": [
    "speech = extract_speech()\n",
    "speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0981282",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(speech)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
